{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb7c7d7",
   "metadata": {},
   "source": [
    "# Group 5 Notebook\n",
    "\n",
    "## Topics:\n",
    "\n",
    "Datasets/Tokenizing (part 1)\n",
    " \n",
    "Models (part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97182f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we are doing NLP, by FAR the best package/tool we can use is Huggingface\n",
    "# https://huggingface.co/docs/transformers/quicktour\n",
    "# pip install transformers datasets tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce991e93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa65301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by selecting a dataset. \n",
    "# I found one that contains (context, question, answer) pairs about wikipedia pages\n",
    "# https://huggingface.co/datasets/lmqg/qa_wiki_t5_large\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lmqg/qa_wiki_t5_large\")\n",
    "# This might take a while to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebfe7eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '54766',\n",
       " 'title': 'Federal government of the United States',\n",
       " 'context': 'The government of the United States of America is the federal government of the republic of fifty states that constitute the United States, as well as one capital district, and several other territories. The federal government is composed of three distinct branches: legislative, executive, and judicial, whose powers are vested by the U.S. Constitution in the Congress, the President, and the federal courts, including the Supreme Court, respectively. The powers and duties of these branches are further defined by acts of Congress, including the creation of executive departments and courts inferior to the Supreme Court.',\n",
       " 'question': 'What is the government of the United States of America?',\n",
       " 'answers': {'text': ['federal government of the republic of fifty states that constitute the United States'],\n",
       "  'answer_start': [54]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f27cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can tokenize our text:\n",
    "# Learn more about BPE here: https://huggingface.co/docs/transformers/tokenizer_summary\n",
    "# And we see the implementation here: https://huggingface.co/course/chapter6/5?fw=pt\n",
    "# Finally we can follow the walkthrough here: https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "\n",
    "# files = [...]\n",
    "# tokenizer.train(files, trainer)\n",
    "\n",
    "# We don't have a list of files easily available. \n",
    "# So instead we can use: https://huggingface.co/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer.train_from_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b10e888",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2080604149.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    my_generator = # TODO\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Task 1: \n",
    "# Train the BPE on our corpus of data\n",
    "\n",
    "# Step 1. \n",
    "# Create an iterator that contains all of our relevant data\n",
    "    # What is relevant? In this example, we'll be given the 'context' and we'll try to predict 'question' and 'answer'\n",
    "    # So our 'corpus' (all of our relevant data) is simply all of the contexts, questions, and answers in our dataset\n",
    "    # How do we create a iterator?\n",
    "    # A list is fine, but in order to save memory we'll use a generator. \n",
    "    # How do we make a generator?\n",
    "    # Like this:\n",
    "# example_generator = (function(x) for x in iterator)\n",
    "# Hint: dataset['train'] is an iterator\n",
    "# Hint: you need to make a function to convert each dataset sample into a single string\n",
    "    \n",
    "def convert_to_single_string(sample):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "my_generator = # TODO\n",
    "next(my_generator) # useful to test, feel free to remove once working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae20394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.\n",
    "# refer to the documentation for tokenizer.train_from_iterator() \n",
    "# https://huggingface.co/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer.train_from_iterator\n",
    "# in order to train our tokenizer \n",
    "tokenizer.train_from_iterator(___TODO____)\n",
    "# This is slow, careful! I encourage you to work on Part 2 (making models) in the meantime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6414a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our tokenizer\n",
    "tokenizer.save(\"qa_wiki_t5_large_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82c824a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our tokenizer from the file, but now with a faster implementation\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"qa_wiki_t5_large_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "124e932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[3442, 6841, 9258, 3487, 3941], [3592, 28210, 7640, 18305]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer([\"the big dog was red\", \"antidisestablishmentarianism\"])\n",
    "# feel free to test your own words here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222b0e4",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## We've created a custom tokenizer, now we need to create a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f40e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We learned about RNNs during out meeting. While we can easily use the PyTorch implementation for RNNs:\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "# We're going to learn by doing, and make one ourselves\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c1e2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the framework for creating an RNN.\n",
    "# Recall that the __init__ function is where we can instantiate layers, and the forward function is where we apply those \n",
    "# layers to our data.\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that an RNN behaves as such:\n",
    "# https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc\n",
    "# At each step the model has two inputs and two outputs\n",
    "# The input x is often the token at that position. The input h is the model's hidden state from all previous tokens.\n",
    "# The output y is the models prediction of the next token. The output h is the model's hidden state to the next token.\n",
    "\n",
    "\n",
    "# Let's have our example sentence be:\n",
    "# The big dog was red\n",
    "\n",
    "# Our model will first process 'the' (if we are tokenizing by each word)\n",
    "# The input x will be the tokenized version of 'the'.  Shape = 1xToken_Dim\n",
    "# The input h, since it is the first token, will be randomly initialized. Shape = 1xHidden_Dim\n",
    "\n",
    "# We will concatenate these two inputs and call this X_Concat. Shape = 1x(Token_Dim + Hidden_Dim)\n",
    "# We will first calculate the hidden state. We will perform a matrix multiply with a weight matrix we will call W_Hidden.\n",
    "# W_Hidden's shape is (Token_dim + Hidden_dim) x Hidden_Dim\n",
    "\n",
    "# We will perform a matrix-vector multiply with W_Hidden and X_Concat. \n",
    "# We will also apply a non-linear function. It is common practice to use tanh or sigmoid. \n",
    "# This gives us our hidden state output, h. \n",
    "# h = sigmoid(W_Hidden * X_Concat)\n",
    "\n",
    "# We will also calculate our output token, Y.\n",
    "# We will perform a matrix-vector multiply with a matrix W_Output.\n",
    "# W_output's shape is (Hidden_dim)x(Output_dim)\n",
    "# Often, but not always, output_dim == token_dim.\n",
    "# y = W_Output * h\n",
    "\n",
    "# We now have our outputs y and h. \n",
    "\n",
    "\n",
    "# The model will now take as an input the returned hidden vector, h, as well as the tokenized word 'big'.\n",
    "# etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2:\n",
    "# Create an RNN that works on some sequence:\n",
    "batch_size = 4\n",
    "sequence_length = 100\n",
    "token_dim = 256\n",
    "input_data = torch.rand(batch_size, sequence_length, token_dim) \n",
    "\n",
    "# Tools needed:\n",
    "# torch.nn.Linear, to create a weight matrix and perform matrix multiplications.\n",
    "# Usage:\n",
    "# __init__():\n",
    "# ...\n",
    "# self.mylayer = torch.nn.Linear(input_dim, output_dim)\n",
    "# ...\n",
    "# forward(x):\n",
    "# ...\n",
    "# (x.shape = Batch, input_dim)\n",
    "# x = self.mylayer(x)\n",
    "# (x.shape = Batch, output_dim)\n",
    "\n",
    "# torch.sigmoid or torch.tanh\n",
    "# Usage:\n",
    "# foward(x):\n",
    "# (x.shape = Batch, dim)\n",
    "# x = torch.sigmoid(x)\n",
    "# (x.shape = Batch, dim)\n",
    "\n",
    "# torch.cat\n",
    "# Usage:\n",
    "# foward(x, hidden):\n",
    "# (x.shape = Batch, token_dim) (hidden.shape = Batch, hidden_dim)\n",
    "# my_new_vec = torch.cat([x, hidden], dim=1)  \n",
    "# (my_new_vec.shape = Batch, (token_dim + hidden_dim))\n",
    "# We say dim=1 so that we concat along the token/hidden dim, instead of the batch dim (dim =0). \n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "        # TODO create weight layers W_hidden and W_output\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        # TODO concatenate  vectors\n",
    "        # TODO apply W_hidden and sigmoid (or tanh)\n",
    "        # TODO apply W_output\n",
    "        return output, hidden\n",
    "\n",
    "hidden_size = __ # up to you!\n",
    "model = MyRNN(token_dim, hidden_size, token_dim)\n",
    "\n",
    "current_hidden = torch.rand(4, hidden_size)\n",
    "for sequence_idx in range(100):\n",
    "    current_token = input_data[:, sequence_idx]\n",
    "    out, current_hidden = model(current_token, current_hidden)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
