{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd1caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6eb6e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset qa_wiki_t5_large (C:/Users/willc/.cache/huggingface/datasets/lmqg___qa_wiki_t5_large/default/0.0.0/f3a1d4d9e366c8e5d66c9329ca2a32b4cb673782cda308c7ef928789d431cfcb)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77132c95305a4799abb74cbdd6f80433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lmqg/qa_wiki_t5_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f78bbd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '54766',\n",
       " 'title': 'Federal government of the United States',\n",
       " 'context': 'The government of the United States of America is the federal government of the republic of fifty states that constitute the United States, as well as one capital district, and several other territories. The federal government is composed of three distinct branches: legislative, executive, and judicial, whose powers are vested by the U.S. Constitution in the Congress, the President, and the federal courts, including the Supreme Court, respectively. The powers and duties of these branches are further defined by acts of Congress, including the creation of executive departments and courts inferior to the Supreme Court.',\n",
       " 'question': 'What is the government of the United States of America?',\n",
       " 'answers': {'text': ['federal government of the republic of fifty states that constitute the United States'],\n",
       "  'answer_start': [54]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5534ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"qa_wiki_t5_large_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7cd55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W', 'i', 'k', 'i', 'p', 'e', 'd', 'i', 'a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer(\"Wikipedia\").tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea1188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.w_h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.w_o = nn.Linear(hidden_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        \n",
    "    def forward(self, input_token, hidden_state):\n",
    "        # B, E\n",
    "        # B, E\n",
    "        comb = torch.cat([input_token, hidden_state], dim=1)\n",
    "        hidden = self.w_h(comb)\n",
    "        out = self.w_o(hidden)\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1166b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(2048, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ada375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f6c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_single_string(sample):\n",
    "#     print(sample)\n",
    "    return sample['title'] + \"[SEP]\" + sample['context'] + \"[ANS]\" +sample['answers']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4edc9eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 7.591909885406494\n",
      "Loss 7.590007781982422\n",
      "Loss 7.589199542999268\n",
      "Loss 7.590047359466553\n",
      "Loss 7.587950229644775\n",
      "Loss 7.587266445159912\n",
      "Loss 7.5871710777282715\n",
      "Loss 7.586010456085205\n",
      "Loss 7.565362453460693\n",
      "Loss 7.568782329559326\n",
      "Loss 7.580281734466553\n",
      "Loss 7.578481674194336\n",
      "Loss 7.57880163192749\n",
      "Loss 7.578470230102539\n",
      "Loss 7.570664882659912\n",
      "Loss 7.5692315101623535\n",
      "Loss 7.568522930145264\n",
      "Loss 7.568769931793213\n",
      "Loss 7.571009159088135\n",
      "Loss 7.578007221221924\n",
      "Loss 7.575453281402588\n",
      "Loss 7.575192928314209\n",
      "Loss 7.575178623199463\n",
      "Loss 7.5735273361206055\n",
      "Loss 7.541576862335205\n",
      "Loss 7.543415069580078\n",
      "Loss 7.538432598114014\n",
      "Loss 7.541182041168213\n",
      "Loss 7.57912540435791\n",
      "Loss 7.578488349914551\n",
      "Loss 7.57764196395874\n",
      "Loss 7.552777290344238\n",
      "Loss 7.5540595054626465\n",
      "Loss 7.554649829864502\n",
      "Loss 7.564908504486084\n",
      "Loss 7.56063175201416\n",
      "Loss 7.561572551727295\n",
      "Loss 7.560378074645996\n",
      "Loss 7.558651447296143\n",
      "Loss 7.556619167327881\n",
      "Loss 7.568521499633789\n",
      "Loss 7.567233562469482\n",
      "Loss 7.5667853355407715\n",
      "Loss 7.563429832458496\n",
      "Loss 7.562612533569336\n",
      "Loss 7.562077045440674\n",
      "Loss 7.5367937088012695\n",
      "Loss 7.539819240570068\n",
      "Loss 7.540322303771973\n",
      "Loss 7.521630764007568\n",
      "Loss 7.551183223724365\n",
      "Loss 7.551222801208496\n",
      "Loss 7.549443244934082\n",
      "Loss 7.546837329864502\n",
      "Loss 7.547436714172363\n",
      "Loss 7.545924663543701\n",
      "Loss 7.538944721221924\n",
      "Loss 7.538450241088867\n",
      "Loss 7.536199569702148\n",
      "Loss 7.534602642059326\n",
      "Loss 7.532786846160889\n",
      "Loss 7.538516998291016\n",
      "Loss 7.537882328033447\n",
      "Loss 7.535005569458008\n",
      "Loss 7.53378438949585\n",
      "Loss 7.532632350921631\n",
      "Loss 7.528438568115234\n",
      "Loss 7.526010513305664\n",
      "Loss 7.52300500869751\n",
      "Loss 7.521799564361572\n",
      "Loss 7.519659996032715\n",
      "Loss 7.517573356628418\n",
      "Loss 7.513607025146484\n",
      "Loss 7.499081134796143\n",
      "Loss 7.495137691497803\n",
      "Loss 7.493221759796143\n",
      "Loss 7.492510795593262\n",
      "Loss 7.4594340324401855\n",
      "Loss 7.488922119140625\n",
      "Loss 7.488246917724609\n",
      "Loss 7.4787678718566895\n",
      "Loss 7.4844465255737305\n",
      "Loss 7.483903408050537\n",
      "Loss 7.474582672119141\n",
      "Loss 7.47269868850708\n",
      "Loss 7.467015266418457\n",
      "Loss 7.461540222167969\n",
      "Loss 7.454861164093018\n",
      "Loss 7.448106288909912\n",
      "Loss 7.440254211425781\n",
      "Loss 7.431787967681885\n",
      "Loss 7.422122001647949\n",
      "Loss 7.402237892150879\n",
      "Loss 7.394748210906982\n",
      "Loss 7.3787150382995605\n",
      "Loss 7.36475944519043\n",
      "Loss 7.3377366065979\n",
      "Loss 7.3211469650268555\n",
      "Loss 7.3173322677612305\n",
      "Loss 7.296967506408691\n",
      "Loss 7.270516395568848\n",
      "Loss 7.239185333251953\n",
      "Loss 7.200272083282471\n",
      "Loss 7.161165714263916\n",
      "Loss 7.113577365875244\n",
      "Loss 7.052164554595947\n",
      "Loss 6.981247901916504\n",
      "Loss 6.888571739196777\n",
      "Loss 6.7828569412231445\n",
      "Loss 6.641397476196289\n",
      "Loss 6.430416584014893\n",
      "Loss 6.139066696166992\n",
      "Loss 5.6986541748046875\n",
      "Loss 5.026032447814941\n",
      "Loss 4.09330415725708\n",
      "Loss 7.401641368865967\n",
      "Loss 4.566973686218262\n",
      "Loss 4.038568019866943\n",
      "Loss 4.419308662414551\n",
      "Loss 4.702731609344482\n",
      "Loss 4.965178489685059\n",
      "Loss 5.093031883239746\n",
      "Loss 5.116391181945801\n",
      "Loss 5.1680097579956055\n",
      "Loss 5.211734294891357\n",
      "Loss 5.215277671813965\n",
      "Loss 5.204354286193848\n",
      "Loss 5.173177242279053\n",
      "Loss 5.13704252243042\n",
      "Loss 5.063532829284668\n",
      "Loss 4.987105846405029\n",
      "Loss 4.940763473510742\n",
      "Loss 4.8202314376831055\n",
      "Loss 4.931774139404297\n",
      "Loss 4.789071083068848\n",
      "Loss 4.67824125289917\n",
      "Loss 4.549262523651123\n",
      "Loss 4.34814977645874\n",
      "Loss 4.230907917022705\n",
      "Loss 4.160260200500488\n",
      "Loss 4.141089916229248\n",
      "Loss 4.179758548736572\n",
      "Loss 4.298213958740234\n",
      "Loss 4.347087860107422\n",
      "Loss 4.322424411773682\n",
      "Loss 4.283301830291748\n",
      "Loss 4.210110187530518\n",
      "Loss 4.08327579498291\n",
      "Loss 4.0195136070251465\n",
      "Loss 4.007415771484375\n",
      "Loss 4.00986385345459\n",
      "Loss 4.005091190338135\n",
      "Loss 4.032890796661377\n",
      "Loss 4.023272514343262\n",
      "Loss 4.361981391906738\n",
      "Loss 4.349675178527832\n",
      "Loss 4.327611923217773\n",
      "Loss 4.231131076812744\n",
      "Loss 4.179485321044922\n",
      "Loss 4.210805416107178\n",
      "Loss 4.1194305419921875\n",
      "Loss 4.141550064086914\n",
      "Loss 3.9927921295166016\n",
      "Loss 4.068374156951904\n",
      "Loss 4.049475193023682\n",
      "Loss 3.8869898319244385\n",
      "Loss 3.9038496017456055\n",
      "Loss 3.8846068382263184\n",
      "Loss 3.879222869873047\n",
      "Loss 3.700575113296509\n",
      "Loss 3.6856727600097656\n",
      "Loss 3.7223570346832275\n",
      "Loss 3.9002554416656494\n",
      "Loss 3.901355266571045\n",
      "Loss 3.900521993637085\n",
      "Loss 3.932468891143799\n",
      "Loss 3.8677918910980225\n",
      "Loss 4.031898498535156\n",
      "Loss 4.107632637023926\n",
      "Loss 4.103532314300537\n",
      "Loss 4.077266693115234\n",
      "Loss 4.073460578918457\n",
      "Loss 3.753021717071533\n",
      "Loss 3.8138370513916016\n",
      "Loss 3.7165327072143555\n",
      "Loss 3.730431079864502\n",
      "Loss 3.6380615234375\n",
      "Loss 3.6437740325927734\n",
      "Loss 3.618600845336914\n",
      "Loss 3.6351089477539062\n",
      "Loss 3.820054769515991\n",
      "Loss 3.81111741065979\n",
      "Loss 3.769257068634033\n",
      "Loss 3.7284674644470215\n",
      "Loss 3.699354887008667\n",
      "Loss 3.7015509605407715\n",
      "Loss 3.691652774810791\n",
      "Loss 3.3554530143737793\n",
      "Loss 3.350372791290283\n",
      "Loss 3.3487939834594727\n",
      "Loss 3.755864381790161\n",
      "Loss 3.633223295211792\n",
      "Loss 3.685248613357544\n",
      "Loss 3.49149227142334\n",
      "Loss 3.546196937561035\n",
      "Loss 3.5731096267700195\n",
      "Loss 3.4899933338165283\n",
      "Loss 3.5449025630950928\n",
      "Loss 3.5528695583343506\n",
      "Loss 3.5454208850860596\n",
      "Loss 4.092313766479492\n",
      "Loss 4.394124984741211\n",
      "Loss 4.362970352172852\n",
      "Loss 4.293217658996582\n",
      "Loss 4.041385173797607\n",
      "Loss 4.033181190490723\n",
      "Loss 4.030908107757568\n",
      "Loss 3.8245439529418945\n",
      "Loss 3.753154754638672\n",
      "Loss 3.9337050914764404\n",
      "Loss 3.8934237957000732\n",
      "Loss 3.912304639816284\n",
      "Loss 3.8951032161712646\n",
      "Loss 4.39479398727417\n",
      "Loss 4.386749744415283\n",
      "Loss 4.376734256744385\n",
      "Loss 3.545027256011963\n",
      "Loss 3.5436713695526123\n",
      "Loss 3.5367305278778076\n",
      "Loss 3.5375726222991943\n",
      "Loss 3.534363269805908\n",
      "Loss 3.519197940826416\n",
      "Loss 3.7678568363189697\n",
      "Loss 3.751803398132324\n",
      "Loss 3.7493174076080322\n",
      "Loss 3.7388834953308105\n",
      "Loss 3.7394487857818604\n",
      "Loss 3.7317771911621094\n",
      "Loss 3.437588691711426\n",
      "Loss 3.4659364223480225\n",
      "Loss 3.432342767715454\n",
      "Loss 3.422999858856201\n",
      "Loss 3.5632405281066895\n",
      "Loss 3.6118228435516357\n",
      "Loss 3.525963544845581\n",
      "Loss 3.548711061477661\n",
      "Loss 3.515064001083374\n",
      "Loss 3.4932708740234375\n",
      "Loss 3.468502998352051\n",
      "Loss 4.484239101409912\n",
      "Loss 4.459776401519775\n",
      "Loss 4.424249172210693\n",
      "Loss 3.5458054542541504\n",
      "Loss 3.5711183547973633\n",
      "Loss 3.561678409576416\n",
      "Loss 3.370443820953369\n",
      "Loss 3.3711860179901123\n",
      "Loss 3.5385775566101074\n",
      "Loss 3.533963680267334\n",
      "Loss 3.5341527462005615\n",
      "Loss 3.5257792472839355\n",
      "Loss 3.667722702026367\n",
      "Loss 3.6113040447235107\n",
      "Loss 3.604395627975464\n",
      "Loss 3.650392770767212\n",
      "Loss 3.6389987468719482\n",
      "Loss 3.583299398422241\n",
      "Loss 3.614374876022339\n",
      "Loss 3.6252031326293945\n",
      "Loss 3.615565299987793\n",
      "Loss 3.626946210861206\n",
      "Loss 3.4678008556365967\n",
      "Loss 3.5194811820983887\n",
      "Loss 3.470829963684082\n",
      "Loss 3.481987237930298\n",
      "Loss 3.495706796646118\n",
      "Loss 3.4810776710510254\n",
      "Loss 3.4629886150360107\n",
      "Loss 3.464738130569458\n",
      "Loss 3.461958646774292\n",
      "Loss 3.5174310207366943\n",
      "Loss 3.4476139545440674\n",
      "Loss 3.5113511085510254\n",
      "Loss 3.4352612495422363\n",
      "Loss 3.3621952533721924\n",
      "Loss 3.3617594242095947\n",
      "Loss 3.3370890617370605\n",
      "Loss 3.3599085807800293\n",
      "Loss 3.4380013942718506\n",
      "Loss 3.444556713104248\n",
      "Loss 3.440979480743408\n",
      "Loss 3.4456424713134766\n",
      "Loss 3.4389636516571045\n",
      "Loss 3.437891960144043\n",
      "Loss 3.4820077419281006\n",
      "Loss 3.447373628616333\n",
      "Loss 3.4728832244873047\n",
      "Loss 3.4763598442077637\n",
      "Loss 3.588919162750244\n",
      "Loss 3.59613037109375\n",
      "Loss 3.415055751800537\n",
      "Loss 3.3925938606262207\n",
      "Loss 3.4298717975616455\n",
      "Loss 3.416273832321167\n",
      "Loss 3.4894533157348633\n",
      "Loss 3.4807145595550537\n",
      "Loss 3.8022406101226807\n",
      "Loss 3.7578439712524414\n",
      "Loss 3.7387349605560303\n",
      "Loss 3.4676353931427\n",
      "Loss 3.4248580932617188\n",
      "Loss 3.2874794006347656\n",
      "Loss 3.3645317554473877\n",
      "Loss 3.595745801925659\n",
      "Loss 3.5703771114349365\n",
      "Loss 3.545360565185547\n",
      "Loss 3.5431389808654785\n",
      "Loss 3.5227465629577637\n",
      "Loss 3.5147757530212402\n",
      "Loss 3.5053107738494873\n",
      "Loss 3.4740848541259766\n",
      "Loss 3.5259079933166504\n",
      "Loss 3.477489471435547\n",
      "Loss 3.468597650527954\n",
      "Loss 3.4454007148742676\n",
      "Loss 3.448951244354248\n",
      "Loss 3.4412050247192383\n",
      "Loss 3.415221691131592\n",
      "Loss 3.444074869155884\n",
      "Loss 3.4253222942352295\n",
      "Loss 3.433239221572876\n",
      "Loss 3.45345139503479\n",
      "Loss 3.4351437091827393\n",
      "Loss 3.5297911167144775\n",
      "Loss 3.5274250507354736\n",
      "Loss 3.529383659362793\n",
      "Loss 3.524470090866089\n",
      "Loss 3.54361629486084\n",
      "Loss 3.5214803218841553\n",
      "Loss 3.5025405883789062\n",
      "Loss 3.5278987884521484\n",
      "Loss 3.499298095703125\n",
      "Loss 3.5055601596832275\n",
      "Loss 3.492281913757324\n",
      "Loss 4.0198516845703125\n",
      "Loss 3.922356128692627\n",
      "Loss 3.7205746173858643\n",
      "Loss 3.7244200706481934\n",
      "Loss 3.66438627243042\n",
      "Loss 3.649977922439575\n",
      "Loss 3.7071707248687744\n",
      "Loss 3.712193727493286\n",
      "Loss 3.6956119537353516\n",
      "Loss 3.6868367195129395\n",
      "Loss 3.6653904914855957\n",
      "Loss 3.6576762199401855\n",
      "Loss 3.3807101249694824\n",
      "Loss 3.388435125350952\n",
      "Loss 3.405891180038452\n",
      "Loss 3.423520803451538\n",
      "Loss 3.391211986541748\n",
      "Loss 3.3912553787231445\n",
      "Loss 3.3842856884002686\n",
      "Loss 3.3807172775268555\n",
      "Loss 3.3921258449554443\n",
      "Loss 3.3831088542938232\n",
      "Loss 3.363245964050293\n",
      "Loss 3.30513858795166\n",
      "Loss 3.3544347286224365\n",
      "Loss 3.3956565856933594\n",
      "Loss 3.3688201904296875\n",
      "Loss 3.389507293701172\n",
      "Loss 3.3719193935394287\n",
      "Loss 3.408914804458618\n",
      "Loss 3.2663137912750244\n",
      "Loss 3.2578349113464355\n",
      "Loss 3.422499418258667\n",
      "Loss 3.489781141281128\n",
      "Loss 4.005235195159912\n",
      "Loss 4.063436031341553\n",
      "Loss 4.0516510009765625\n",
      "Loss 3.452129364013672\n",
      "Loss 3.4642484188079834\n",
      "Loss 3.4464504718780518\n",
      "Loss 3.447719097137451\n",
      "Loss 3.4380605220794678\n",
      "Loss 3.4281532764434814\n",
      "Loss 3.40407133102417\n",
      "Loss 3.396404266357422\n",
      "Loss 3.4071273803710938\n",
      "Loss 3.398942232131958\n",
      "Loss 3.4089505672454834\n",
      "Loss 3.3983469009399414\n",
      "Loss 3.4131903648376465\n",
      "Loss 3.742950916290283\n",
      "Loss 3.7342002391815186\n",
      "Loss 3.7898154258728027\n",
      "Loss 3.7868666648864746\n",
      "Loss 3.775742292404175\n",
      "Loss 3.820814847946167\n",
      "Loss 3.8717877864837646\n",
      "Loss 3.8462588787078857\n",
      "Loss 3.8442978858947754\n",
      "Loss 3.6381447315216064\n",
      "Loss 3.652562379837036\n",
      "Loss 3.636721134185791\n",
      "Loss 3.6243393421173096\n",
      "Loss 3.601327657699585\n",
      "Loss 3.513350248336792\n",
      "Loss 3.5288922786712646\n",
      "Loss 3.471214771270752\n",
      "Loss 3.405430316925049\n",
      "Loss 3.417694091796875\n",
      "Loss 3.4237918853759766\n",
      "Loss 3.4456565380096436\n",
      "Loss 3.443476915359497\n",
      "Loss 3.523341655731201\n",
      "Loss 3.4810616970062256\n",
      "Loss 3.4805636405944824\n",
      "Loss 3.5558881759643555\n",
      "Loss 3.5548977851867676\n",
      "Loss 3.5584044456481934\n",
      "Loss 3.5523130893707275\n",
      "Loss 3.523897886276245\n",
      "Loss 3.5436465740203857\n",
      "Loss 3.556056499481201\n",
      "Loss 3.5553925037384033\n",
      "Loss 3.5579311847686768\n",
      "Loss 3.5478005409240723\n",
      "Loss 3.6683433055877686\n",
      "Loss 3.7137258052825928\n",
      "Loss 3.6404693126678467\n",
      "Loss 3.3033926486968994\n",
      "Loss 3.308988332748413\n",
      "Loss 3.2902746200561523\n",
      "Loss 3.2774672508239746\n",
      "Loss 3.355311632156372\n",
      "Loss 3.356999635696411\n",
      "Loss 3.3540241718292236\n",
      "Loss 3.3478305339813232\n",
      "Loss 3.35786771774292\n",
      "Loss 3.4453463554382324\n",
      "Loss 3.4420855045318604\n",
      "Loss 3.450411081314087\n",
      "Loss 3.449533462524414\n",
      "Loss 3.454936981201172\n",
      "Loss 3.433567523956299\n",
      "Loss 3.4268109798431396\n",
      "Loss 3.4527249336242676\n",
      "Loss 3.616567850112915\n",
      "Loss 3.6213581562042236\n",
      "Loss 3.823533296585083\n",
      "Loss 3.908074140548706\n",
      "Loss 3.748387575149536\n",
      "Loss 3.7687509059906006\n",
      "Loss 3.75130558013916\n",
      "Loss 3.755927324295044\n",
      "Loss 3.748429536819458\n",
      "Loss 3.7468454837799072\n",
      "Loss 3.6954424381256104\n",
      "Loss 3.659517526626587\n",
      "Loss 3.6762993335723877\n",
      "Loss 3.6703338623046875\n",
      "Loss 4.259600639343262\n",
      "Loss 4.301809310913086\n",
      "Loss 4.193480491638184\n",
      "Loss 3.9016261100769043\n",
      "Loss 3.878221035003662\n",
      "Loss 3.8874757289886475\n",
      "Loss 3.8772313594818115\n",
      "Loss 3.6771137714385986\n",
      "Loss 3.7509756088256836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m             opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, model)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#                 print(y_true)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m                 loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m crit(y_pred, y_true\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m/\u001b[39m(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m             \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m             opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\.conda\\envs\\496\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\496\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(dataset, model): \n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        for sample in dataset['train']:\n",
    "            opt.zero_grad()\n",
    "            x = convert_to_single_string(sample)\n",
    "            x = fast_tokenizer(x, return_tensors='pt')['input_ids']\n",
    "            x = torch.nn.functional.one_hot(x, 2048)\n",
    "            \n",
    "            state_h = torch.zeros(1, 64)\n",
    "            loss = torch.tensor([0.0])\n",
    "            for tok_idx in range(x.size()[1]-1):\n",
    "                curr_token = x[:, tok_idx]\n",
    "                y_pred, state_h = model(curr_token, state_h)\n",
    "                y_true = x[:, tok_idx+1]\n",
    "#                 print(y_true)\n",
    "                loss += crit(y_pred, y_true.float())/(x.size()[1])\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            print(\"Loss\", loss.item())\n",
    "            \n",
    "            \n",
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fbb731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fast_tokenizer(\"Describe and analyze an ecient algorithm that determines, given a legal arrangement of standard pieces on a standard chess board, which player will win at chess from the given starting position if both players play perfectly. [Hint: There is a trivial one-line solution!]. (a) Identify (or write) a song that requires â‡¥(n3) time to sing the first n verses.\", return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68e05d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.functional.one_hot(x, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1dbf579",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_h = torch.zeros(1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80d5a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok_idx in range(x.size()[1]-1):\n",
    "    curr_token = x[:, tok_idx]\n",
    "    y_pred, state_h = model(curr_token, state_h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e83dee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for i in range(100):\n",
    "    out, state_h = model(torch.nn.functional.one_hot(fast_tokenizer(\"[ANS]\", return_tensors='pt')['input_ids'], 2048)[0], state_h)\n",
    "    outs.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "972351b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2048])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(fast_tokenizer(\"[ANS]\", return_tensors='pt')['input_ids'], 2048).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b44127e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([74])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34805d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-2.4361, -3.0646,  0.2659,  ..., -2.6925, -2.9925, -2.9834]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4337, -3.0616,  0.2656,  ..., -2.6897, -2.9895, -2.9803]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4314, -3.0586,  0.2653,  ..., -2.6870, -2.9866, -2.9773]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4292, -3.0557,  0.2650,  ..., -2.6844, -2.9837, -2.9744]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4270, -3.0529,  0.2648,  ..., -2.6818, -2.9809, -2.9715]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4249, -3.0501,  0.2645,  ..., -2.6793, -2.9782, -2.9688]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4228, -3.0475,  0.2643,  ..., -2.6769, -2.9756, -2.9661]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4208, -3.0449,  0.2640,  ..., -2.6746, -2.9730, -2.9635]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4189, -3.0424,  0.2638,  ..., -2.6723, -2.9706, -2.9609]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4170, -3.0399,  0.2636,  ..., -2.6701, -2.9682, -2.9584]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4151, -3.0375,  0.2633,  ..., -2.6679, -2.9658, -2.9560]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4133, -3.0352,  0.2631,  ..., -2.6658, -2.9635, -2.9537]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4116, -3.0330,  0.2629,  ..., -2.6638, -2.9613, -2.9514]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4099, -3.0308,  0.2627,  ..., -2.6618, -2.9592, -2.9492]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4083, -3.0287,  0.2625,  ..., -2.6598, -2.9571, -2.9471]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4067, -3.0266,  0.2623,  ..., -2.6580, -2.9550, -2.9450]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4051, -3.0246,  0.2621,  ..., -2.6562, -2.9530, -2.9430]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4036, -3.0226,  0.2620,  ..., -2.6544, -2.9511, -2.9410]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4021, -3.0207,  0.2618,  ..., -2.6527, -2.9492, -2.9391]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.4007, -3.0189,  0.2616,  ..., -2.6510, -2.9474, -2.9372]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3993, -3.0171,  0.2614,  ..., -2.6494, -2.9457, -2.9354]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3980, -3.0153,  0.2613,  ..., -2.6478, -2.9439, -2.9336]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3967, -3.0136,  0.2611,  ..., -2.6462, -2.9423, -2.9319]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3954, -3.0120,  0.2610,  ..., -2.6448, -2.9406, -2.9303]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3941, -3.0104,  0.2608,  ..., -2.6433, -2.9391, -2.9287]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3929, -3.0088,  0.2607,  ..., -2.6419, -2.9375, -2.9271]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3918, -3.0073,  0.2605,  ..., -2.6405, -2.9360, -2.9256]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3906, -3.0058,  0.2604,  ..., -2.6392, -2.9346, -2.9241]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3895, -3.0044,  0.2603,  ..., -2.6379, -2.9332, -2.9226]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3884, -3.0030,  0.2601,  ..., -2.6366, -2.9318, -2.9212]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3874, -3.0017,  0.2600,  ..., -2.6354, -2.9305, -2.9198]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3864, -3.0003,  0.2599,  ..., -2.6342, -2.9292, -2.9185]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3854, -2.9991,  0.2598,  ..., -2.6330, -2.9279, -2.9172]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3844, -2.9978,  0.2596,  ..., -2.6319, -2.9267, -2.9160]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3835, -2.9966,  0.2595,  ..., -2.6308, -2.9255, -2.9148]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3826, -2.9954,  0.2594,  ..., -2.6298, -2.9243, -2.9136]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3817, -2.9943,  0.2593,  ..., -2.6287, -2.9232, -2.9124]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3808, -2.9932,  0.2592,  ..., -2.6277, -2.9221, -2.9113]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3800, -2.9921,  0.2591,  ..., -2.6267, -2.9211, -2.9102]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3792, -2.9911,  0.2590,  ..., -2.6258, -2.9200, -2.9091]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3784, -2.9900,  0.2589,  ..., -2.6249, -2.9190, -2.9081]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3776, -2.9890,  0.2588,  ..., -2.6240, -2.9180, -2.9071]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3769, -2.9881,  0.2587,  ..., -2.6231, -2.9171, -2.9061]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3762, -2.9871,  0.2586,  ..., -2.6222, -2.9162, -2.9052]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3755, -2.9862,  0.2586,  ..., -2.6214, -2.9153, -2.9043]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3748, -2.9853,  0.2585,  ..., -2.6206, -2.9144, -2.9034]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3741, -2.9845,  0.2584,  ..., -2.6198, -2.9135, -2.9025]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3735, -2.9836,  0.2583,  ..., -2.6191, -2.9127, -2.9017]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3728, -2.9828,  0.2582,  ..., -2.6183, -2.9119, -2.9008]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3722, -2.9820,  0.2582,  ..., -2.6176, -2.9111, -2.9000]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3716, -2.9813,  0.2581,  ..., -2.6169, -2.9104, -2.8993]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3710, -2.9805,  0.2580,  ..., -2.6162, -2.9096, -2.8985]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3705, -2.9798,  0.2580,  ..., -2.6156, -2.9089, -2.8978]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3699, -2.9791,  0.2579,  ..., -2.6149, -2.9082, -2.8971]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3694, -2.9784,  0.2578,  ..., -2.6143, -2.9075, -2.8964]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3689, -2.9777,  0.2578,  ..., -2.6137, -2.9069, -2.8957]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3684, -2.9771,  0.2577,  ..., -2.6131, -2.9062, -2.8950]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3679, -2.9764,  0.2577,  ..., -2.6125, -2.9056, -2.8944]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3674, -2.9758,  0.2576,  ..., -2.6120, -2.9050, -2.8938]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3669, -2.9752,  0.2575,  ..., -2.6114, -2.9044, -2.8932]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3665, -2.9747,  0.2575,  ..., -2.6109, -2.9039, -2.8926]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3661, -2.9741,  0.2574,  ..., -2.6104, -2.9033, -2.8920]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3656, -2.9735,  0.2574,  ..., -2.6099, -2.9028, -2.8915]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3652, -2.9730,  0.2573,  ..., -2.6094, -2.9022, -2.8909]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3648, -2.9725,  0.2573,  ..., -2.6089, -2.9017, -2.8904]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3644, -2.9720,  0.2572,  ..., -2.6085, -2.9012, -2.8899]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3641, -2.9715,  0.2572,  ..., -2.6080, -2.9007, -2.8894]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3637, -2.9710,  0.2571,  ..., -2.6076, -2.9003, -2.8889]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3633, -2.9706,  0.2571,  ..., -2.6072, -2.8998, -2.8884]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3630, -2.9701,  0.2571,  ..., -2.6068, -2.8994, -2.8880]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3626, -2.9697,  0.2570,  ..., -2.6064, -2.8989, -2.8876]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3623, -2.9692,  0.2570,  ..., -2.6060, -2.8985, -2.8871]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3620, -2.9688,  0.2569,  ..., -2.6056, -2.8981, -2.8867]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3617, -2.9684,  0.2569,  ..., -2.6053, -2.8977, -2.8863]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3614, -2.9680,  0.2569,  ..., -2.6049, -2.8973, -2.8859]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3611, -2.9676,  0.2568,  ..., -2.6046, -2.8970, -2.8855]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3608, -2.9673,  0.2568,  ..., -2.6042, -2.8966, -2.8851]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3605, -2.9669,  0.2568,  ..., -2.6039, -2.8962, -2.8848]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3602, -2.9666,  0.2567,  ..., -2.6036, -2.8959, -2.8844]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3600, -2.9662,  0.2567,  ..., -2.6033, -2.8956, -2.8841]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3597, -2.9659,  0.2567,  ..., -2.6030, -2.8952, -2.8838]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3595, -2.9656,  0.2566,  ..., -2.6027, -2.8949, -2.8834]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3592, -2.9653,  0.2566,  ..., -2.6024, -2.8946, -2.8831]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3590, -2.9650,  0.2566,  ..., -2.6021, -2.8943, -2.8828]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3588, -2.9647,  0.2566,  ..., -2.6019, -2.8940, -2.8825]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3586, -2.9644,  0.2565,  ..., -2.6016, -2.8937, -2.8822]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3583, -2.9641,  0.2565,  ..., -2.6014, -2.8935, -2.8819]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3581, -2.9638,  0.2565,  ..., -2.6011, -2.8932, -2.8817]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3579, -2.9636,  0.2565,  ..., -2.6009, -2.8929, -2.8814]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3577, -2.9633,  0.2564,  ..., -2.6006, -2.8927, -2.8811]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3575, -2.9631,  0.2564,  ..., -2.6004, -2.8924, -2.8809]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3574, -2.9628,  0.2564,  ..., -2.6002, -2.8922, -2.8806]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3572, -2.9626,  0.2564,  ..., -2.6000, -2.8920, -2.8804]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3570, -2.9624,  0.2563,  ..., -2.5998, -2.8917, -2.8802]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3568, -2.9621,  0.2563,  ..., -2.5996, -2.8915, -2.8800]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3567, -2.9619,  0.2563,  ..., -2.5994, -2.8913, -2.8797]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3565, -2.9617,  0.2563,  ..., -2.5992, -2.8911, -2.8795]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3563, -2.9615,  0.2563,  ..., -2.5990, -2.8909, -2.8793]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3562, -2.9613,  0.2562,  ..., -2.5988, -2.8907, -2.8791]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-2.3560, -2.9611,  0.2562,  ..., -2.5986, -2.8905, -2.8789]],\n",
       "        grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9bcd18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "for o in outs:\n",
    "    print(fast_tokenizer.decode(o.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84883054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
