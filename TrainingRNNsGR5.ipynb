{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e2b3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d155c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset qa_wiki_t5_large (/Users/simoneschwaighart/.cache/huggingface/datasets/lmqg___qa_wiki_t5_large/default/0.0.0/f3a1d4d9e366c8e5d66c9329ca2a32b4cb673782cda308c7ef928789d431cfcb)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b5e0976fa446d8b2ea022b890ddee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lmqg/qa_wiki_t5_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1181b639",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[0;32m----> 3\u001b[0m fast_tokenizer \u001b[38;5;241m=\u001b[39m PreTrainedTokenizerFast(\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa_wiki_t5_large_tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_file' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file-\"qa_wiki_t5_large_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8367c122",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fast_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfast_tokenizer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat were you for Halloween\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fast_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "fast_tokenizer(\"What were you for Halloween\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0b5ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.w_h = nn.Linear(input_size, hidden_size)\n",
    "        self.w_o = nn.Linear(hidden_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def forward(self, input_token, hidden_state):\n",
    "        #B, E\n",
    "        #B, E\n",
    "        comb = torch.cat(input_token, hidden_state, dim-1)\n",
    "        hidden = self.w_h(comb)\n",
    "        out = self.w_o(hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a6533c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(2048, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c24ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_single_string(sample):\n",
    "    return sample['title'] + \"[SEP]\" + sample['context'] + \"[ANS]\" + sample['answers']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49a32435",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'Adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(dataset, model):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'Adam'"
     ]
    }
   ],
   "source": [
    "def train(dataset, model):\n",
    "    model.train()\n",
    "    optimizer = torch.Adam(model.parameters(), lr-0.0001)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        for sample in dataset['train']:\n",
    "            x = convert_to_single_string(sample)\n",
    "            x = fast_tokenizer(x, return_tensors='pt')['input_ids']\n",
    "            x = torch.nn.funcitonal.one_hot(x, 2048)\n",
    "            \n",
    "            state_h = torch.zeros(1, 64)\n",
    "            loss = torch.tensor([0, 0])\n",
    "            for tok_idx in range(x.size()[1]-1):\n",
    "                curr_token = x[:, tok_idx]\n",
    "                y_pred, state_h = model(curr_token, state_h)\n",
    "                y_true = x[:, tok_idx+1]\n",
    "                loss += crit(y_pred, y_true.float())/(x.size()[1])\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            print(\"Loss\", loss.item())\n",
    "            \n",
    "train(dataset, model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "770a6bee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(\u001b[43mx\u001b[49m, \u001b[38;5;241m2048\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.nn.functional.one_hot(x, 2048)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
